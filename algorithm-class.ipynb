{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "b3d191e52b7a1e5633ead23de51b2338a6fd1a7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e099477cd74a5016f91fae49539dcb707ea90ff9"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FFNN:\n",
    "    #self.dataset = Path('../input/New_sounds/Three Sounds')\n",
    "    def __init__(self):\n",
    "        self.dataset = Path('../input/three sounds/Three Sounds')\n",
    "        self.df = pd.DataFrame()\n",
    "        self.labels = []\n",
    "        self.getlabels()\n",
    "        self.one_hot_encode(self.df,self.labels)\n",
    "        #self.inputs = Input\n",
    "        #self.df['feats'] = self.df.feats_raw.apply(preprocess)\n",
    "        #self.feats_extraction(dataset)\n",
    "        self.get_features()\n",
    "        self.apply_function()\n",
    "        self.hotencoding()\n",
    "        self.data_split()\n",
    "        \n",
    "        \n",
    "            # a layer instance is callable on a tensor, and returns a tensor\n",
    "#         x = Dense(64, activation='relu')(self.inputs)\n",
    "#         x = Dense(64, activation='relu')(x)\n",
    "#         x = Dense(64, activation='relu')(x)\n",
    "#         self.predictions = Dense(self.Output_num, activation='softmax')(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(128, activation='relu',kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "        self.model.add(Dropout(0.30))\n",
    "        self.model.add(Dense(64, activation='relu',kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "        self.model.add(Dropout(0.20))\n",
    "        self.model.add(Dense(32, activation='relu',kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "        self.model.add(Dropout(0.10))\n",
    "        self.model.add(Dense(self.Output_num, activation='softmax'))\n",
    "        self.build_model()\n",
    "        #self.graphs()\n",
    "        \n",
    "    \n",
    "    def getlabels(self):\n",
    "        self.labels = [folder.name for folder in self.dataset.glob('*')]\n",
    "        frame_files, frame_labels = [], []\n",
    "        for folder in self.dataset.glob('*'):\n",
    "            files = [path for path in folder.glob('*')]\n",
    "            frame_files.extend(files)\n",
    "            frame_labels.extend([folder.name] * len(files))\n",
    "        frame = [frame_files, frame_labels]\n",
    "        self.df['Path'] = frame_files\n",
    "        self.df['Label'] = frame_labels\n",
    "    \n",
    "    def apply_function(self):\n",
    "        self.df['feats'] = self.df.feats_raw.apply(self.pre_process)\n",
    "        \n",
    "    \n",
    "    #######################Feature Extraction######################\n",
    "    def feats_extraction(self,path):\n",
    "        raw_signal, sample_rate = librosa.load(path)\n",
    "        mfccs = librosa.feature.mfcc(raw_signal, sample_rate) # 20 features (default)\n",
    "        chroma = librosa.feature.chroma_stft(raw_signal, sample_rate) # 12 features\n",
    "        stft = np.abs(librosa.stft(raw_signal))\n",
    "        contrast = librosa.feature.spectral_contrast(S=stft, sr=sample_rate) # 7 features\n",
    "        return np.concatenate([mfccs, chroma, contrast])\n",
    "    \n",
    "    def get_features(self):\n",
    "        self.df['feats_raw'] = self.df.Path.apply(self.feats_extraction)\n",
    "        \n",
    "    #######################Feature Per File#########################\n",
    "    def pre_process(self,feature_matrix):\n",
    "        feats = [\n",
    "            feature_matrix.mean(axis=1),\n",
    "            feature_matrix.std(axis=1),\n",
    "            feature_matrix.min(axis=1),\n",
    "            feature_matrix.max(axis=1),\n",
    "            np.median(feature_matrix, axis=1)\n",
    "        ]\n",
    "        return np.stack(feats).flatten().astype(np.float32)\n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "    #######################Feature Per File#########################\n",
    "    def one_hot_encode(self, df, labels):\n",
    "        encoded_labels = [np.zeros(len(self.labels), dtype='float32') for i in range(len(self.df))]\n",
    "        for idx, val in enumerate(self.df['Label']):\n",
    "            encoded_labels[idx][self.labels.index(val)] = 1\n",
    "        return encoded_labels\n",
    "    \n",
    "    def hotencoding(self):\n",
    "        self.df['Target'] = self.one_hot_encode(self.df, self.labels)\n",
    "        self.Output_num = len(self.df['Target'][0])\n",
    "    \n",
    "    \n",
    "    #######################Splitting the self.dataset#########################\n",
    "#     num1 = float(input(\"Percentage for training: \"))\n",
    "#     num2 = float(input(\"Percentage for validation: \"))\n",
    "    \n",
    "#     num2 = num1+num2\n",
    "#     num3 = 1 - num2\n",
    "    def data_split(self):\n",
    "        indexes = np.arange(len(self.df))\n",
    "        np.random.seed(4) #to ensure values say in the same set during each run and not mix up with other parameters\n",
    "        np.random.shuffle(indexes)\n",
    "\n",
    "        val_pivot = int(0.7*len(self.df))\n",
    "        test_pivot = int(0.85*len(self.df))\n",
    "\n",
    "    #     val_pivot = int(num1*len(self.df))\n",
    "    #     test_pivot = int(num2*len(self.df))\n",
    "\n",
    "        train_indexes = indexes[0:val_pivot] # 70% of self.dataset\n",
    "        val_indexes = indexes[val_pivot:test_pivot] # 15% of self.dataset\n",
    "        test_intexes = indexes[test_pivot:] # 15% of self.dataset\n",
    "\n",
    "        self.df_train = self.df.iloc[train_indexes].reset_index(drop=True)\n",
    "        self.df_val = self.df.iloc[val_indexes].reset_index(drop=True)\n",
    "        self.df_test = self.df.iloc[test_intexes].reset_index(drop=True)\n",
    "\n",
    "        self.train_x, self.train_y = np.stack(self.df_train['feats']), np.stack(self.df_train['Target'])\n",
    "        self.val_x, self.val_y = np.stack(self.df_val['feats']), np.stack(self.df_val['Target'])\n",
    "        self.test_x, self.test_y = np.stack(self.df_test['feats']), np.stack(self.df_test['Target'])\n",
    "        \n",
    "        #self.inputs = Input(shape=(self.train_x.shape[1],))\n",
    "    \n",
    "    #######################Building the model#########################\n",
    "   \n",
    "    \n",
    "    # This returns a tensor\n",
    "    \n",
    "\n",
    "    def build_model(self):\n",
    "        # self.predictions = Dense(2, activation='softmax')(self.inputs)\n",
    "\n",
    "        # This creates a model that includes\n",
    "        # the Input layer and three Dense layers\n",
    "#         model = Model(inputs=self.inputs, outputs=self.predictions)\n",
    "#         model.compile(optimizer='rmsprop',\n",
    "#                   loss='categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "        \n",
    "        self.model.compile(optimizer='rmsprop',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        #print(model.summary())\n",
    "\n",
    "        self.results_val = self.model.fit(self.train_x, self.train_y, validation_data=(self.val_x,self.val_y), epochs=200, batch_size=64) # starts training with validating\n",
    "        print(self.model.summary())\n",
    "#    #######################Plotting the loss###########################\n",
    "#    x_axis = np.arange(200)\n",
    "#    \n",
    "    def graphs(self):\n",
    "        x_axis = np.arange(200)\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(x_axis, self.results_val.history['loss'], x_axis, self.results_val.history['val_loss'])\n",
    "        plt.title('Training Vs Training with Validation')\n",
    "        plt.gca().legend(('Loss','Validation Loss'))\n",
    "        plt.ylabel('Loss')\n",
    "   \n",
    "   \n",
    "   #######################Plotting the accuracy#######################\n",
    "        x_axis = np.arange(200)\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(x_axis, self.results_val.history['acc'], x_axis, self.results_val.history['val_acc'])\n",
    "        plt.gca().legend(('Accuracy','Validation Accuracy'))\n",
    "        plt.ylabel('Accuracy')\n",
    "   \n",
    "        plt.show()\n",
    "    \n",
    "    def graphwindow(self):\n",
    "         #Tab1 = self.TNotebook1_t1\n",
    "#         ent1t1 = tk.Entry(Tab1)\n",
    "#         ent1t1.pack()\n",
    "        \n",
    "        fig = plt.figure(figsize=(5,5), dpi=100)\n",
    "        a = f.add_subplot(111)\n",
    "        #a.plot(self.graphs())\n",
    "        a.plot(x_axis, self.results_val.history['loss'], x_axis, self.results_val.history['val_loss'])\n",
    "        a.title('Training Vs Training with Validation')\n",
    "        a.gca().legend(('Loss','Validation Loss'))\n",
    "        a.ylabel('Loss')\n",
    "\n",
    "        canvas = FigureCanvasTkAgg(fig, self.main_g.TNotebook1_t2)\n",
    "        canvas.draw()\n",
    "        canvas.get_tk_widget().pack(side=tk.BOTTOM, fill=tk.BOTH, expand=True)\n",
    "   \n",
    "        #plt.plot(x_axis, results_val.history['loss'], x_axis, results_val.history['val_loss'])\n",
    "#    \n",
    "#    #######################Plotting the accuracy#######################\n",
    "#    #x_axis = np.arange(200)\n",
    "#    #plt.plot(x_axis, results_val.history['acc'], x_axis, results_val.history['val_acc'])\n",
    "#    \n",
    "#    #######################Evaluating the model#######################\n",
    "#    test_loss, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "#    print('Test accuracy:', test_acc)\n",
    "#    \n",
    "#    #######################Predicting self.labels with the model#################\n",
    "#    self.predictions = model.predict(test_x)\n",
    "#    np.argmax(self.predictions[0])\n",
    "#    \n",
    "#    #######################Saving the model#################################\n",
    "#    model_json = model.to_json()\n",
    "#    with open(\"model.json\", \"w\") as json_file:\n",
    "#        json_file.write(model_json)\n",
    "#    # serialize weights to Hself.df5\n",
    "#    model.save_weights(\"model.h5\")\n",
    "#    print(\"Saved model to disk\")\n",
    "#    \n",
    "#    \n",
    "#    \n",
    "#    # load json and create model\n",
    "#    json_file = open('model.json', 'r')\n",
    "#    loaded_model_json = json_file.read()\n",
    "#    json_file.close()\n",
    "#    loaded_model = model_from_json(loaded_model_json)\n",
    "#    # load weights into new model\n",
    "#    loaded_model.load_weights(\"model.h5\")\n",
    "#    print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "c5134531bae4f28a92ab9e35022a7204b6549792"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/librosa/core/pitch.py:145: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn('Trying to estimate tuning from empty frequency set.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 779 samples, validate on 167 samples\n",
      "Epoch 1/200\n",
      "779/779 [==============================] - 0s 446us/step - loss: 0.7609 - acc: 0.7060 - val_loss: 0.3310 - val_acc: 0.8563\n",
      "Epoch 2/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.3818 - acc: 0.8588 - val_loss: 0.1992 - val_acc: 0.9341\n",
      "Epoch 3/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.3074 - acc: 0.8858 - val_loss: 0.1116 - val_acc: 0.9641\n",
      "Epoch 4/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.2258 - acc: 0.9268 - val_loss: 0.1354 - val_acc: 0.9521\n",
      "Epoch 5/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.2157 - acc: 0.9217 - val_loss: 0.1230 - val_acc: 0.9521\n",
      "Epoch 6/200\n",
      "779/779 [==============================] - 0s 43us/step - loss: 0.2250 - acc: 0.9268 - val_loss: 0.1041 - val_acc: 0.9581\n",
      "Epoch 7/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.2046 - acc: 0.9358 - val_loss: 0.0735 - val_acc: 0.9940\n",
      "Epoch 8/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1735 - acc: 0.9422 - val_loss: 0.0517 - val_acc: 0.9820\n",
      "Epoch 9/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.1361 - acc: 0.9615 - val_loss: 0.0437 - val_acc: 0.9820\n",
      "Epoch 10/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1492 - acc: 0.9474 - val_loss: 0.2094 - val_acc: 0.9222\n",
      "Epoch 11/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.1546 - acc: 0.9422 - val_loss: 0.0737 - val_acc: 0.9641\n",
      "Epoch 12/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1322 - acc: 0.9564 - val_loss: 0.0226 - val_acc: 0.9940\n",
      "Epoch 13/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.1343 - acc: 0.9589 - val_loss: 0.0289 - val_acc: 1.0000\n",
      "Epoch 14/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1248 - acc: 0.9589 - val_loss: 0.0318 - val_acc: 0.9880\n",
      "Epoch 15/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1153 - acc: 0.9602 - val_loss: 0.0207 - val_acc: 1.0000\n",
      "Epoch 16/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1059 - acc: 0.9615 - val_loss: 0.0387 - val_acc: 0.9880\n",
      "Epoch 17/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1187 - acc: 0.9551 - val_loss: 0.0584 - val_acc: 0.9641\n",
      "Epoch 18/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.1177 - acc: 0.9602 - val_loss: 0.0921 - val_acc: 0.9641\n",
      "Epoch 19/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1193 - acc: 0.9564 - val_loss: 0.0566 - val_acc: 0.9760\n",
      "Epoch 20/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1255 - acc: 0.9589 - val_loss: 0.0771 - val_acc: 0.9641\n",
      "Epoch 21/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0889 - acc: 0.9692 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "Epoch 22/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1009 - acc: 0.9653 - val_loss: 0.0297 - val_acc: 0.9940\n",
      "Epoch 23/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.1105 - acc: 0.9615 - val_loss: 0.0370 - val_acc: 0.9880\n",
      "Epoch 24/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.1080 - acc: 0.9589 - val_loss: 0.1060 - val_acc: 0.9521\n",
      "Epoch 25/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0854 - acc: 0.9718 - val_loss: 0.0309 - val_acc: 0.9820\n",
      "Epoch 26/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0771 - acc: 0.9718 - val_loss: 0.0242 - val_acc: 0.9880\n",
      "Epoch 27/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0810 - acc: 0.9628 - val_loss: 0.0159 - val_acc: 1.0000\n",
      "Epoch 28/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0656 - acc: 0.9769 - val_loss: 0.0137 - val_acc: 1.0000\n",
      "Epoch 29/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0863 - acc: 0.9666 - val_loss: 0.0642 - val_acc: 0.9641\n",
      "Epoch 30/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0944 - acc: 0.9564 - val_loss: 0.0257 - val_acc: 0.9880\n",
      "Epoch 31/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0913 - acc: 0.9679 - val_loss: 0.0228 - val_acc: 0.9940\n",
      "Epoch 32/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0916 - acc: 0.9730 - val_loss: 0.0455 - val_acc: 0.9820\n",
      "Epoch 33/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0739 - acc: 0.9743 - val_loss: 0.0231 - val_acc: 0.9940\n",
      "Epoch 34/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0719 - acc: 0.9718 - val_loss: 0.0429 - val_acc: 0.9760\n",
      "Epoch 35/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0766 - acc: 0.9718 - val_loss: 0.0410 - val_acc: 0.9760\n",
      "Epoch 36/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0652 - acc: 0.9769 - val_loss: 0.0597 - val_acc: 0.9760\n",
      "Epoch 37/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0891 - acc: 0.9705 - val_loss: 0.0266 - val_acc: 0.9820\n",
      "Epoch 38/200\n",
      "779/779 [==============================] - 0s 41us/step - loss: 0.0586 - acc: 0.9743 - val_loss: 0.0299 - val_acc: 0.9820\n",
      "Epoch 39/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0530 - acc: 0.9782 - val_loss: 0.0248 - val_acc: 0.9880\n",
      "Epoch 40/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0549 - acc: 0.9795 - val_loss: 0.0251 - val_acc: 0.9880\n",
      "Epoch 41/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0741 - acc: 0.9769 - val_loss: 0.0215 - val_acc: 0.9880\n",
      "Epoch 42/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0605 - acc: 0.9795 - val_loss: 0.0414 - val_acc: 0.9820\n",
      "Epoch 43/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0650 - acc: 0.9769 - val_loss: 0.0230 - val_acc: 0.9880\n",
      "Epoch 44/200\n",
      "779/779 [==============================] - 0s 42us/step - loss: 0.0388 - acc: 0.9833 - val_loss: 0.0396 - val_acc: 0.9880\n",
      "Epoch 45/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0495 - acc: 0.9833 - val_loss: 0.0215 - val_acc: 0.9880\n",
      "Epoch 46/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.1108 - acc: 0.9769 - val_loss: 0.0468 - val_acc: 0.9760\n",
      "Epoch 47/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0454 - acc: 0.9846 - val_loss: 0.0295 - val_acc: 0.9880\n",
      "Epoch 48/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0640 - acc: 0.9730 - val_loss: 0.0191 - val_acc: 0.9880\n",
      "Epoch 49/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0405 - acc: 0.9833 - val_loss: 0.0297 - val_acc: 0.9880\n",
      "Epoch 50/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0586 - acc: 0.9782 - val_loss: 0.0547 - val_acc: 0.9760\n",
      "Epoch 51/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0675 - acc: 0.9769 - val_loss: 0.0376 - val_acc: 0.9880\n",
      "Epoch 52/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0324 - acc: 0.9910 - val_loss: 0.0425 - val_acc: 0.9820\n",
      "Epoch 53/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0499 - acc: 0.9846 - val_loss: 0.0413 - val_acc: 0.9880\n",
      "Epoch 54/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0510 - acc: 0.9782 - val_loss: 0.0528 - val_acc: 0.9820\n",
      "Epoch 55/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0287 - acc: 0.9910 - val_loss: 0.0419 - val_acc: 0.9880\n",
      "Epoch 56/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0670 - acc: 0.9769 - val_loss: 0.0671 - val_acc: 0.9701\n",
      "Epoch 57/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0499 - acc: 0.9795 - val_loss: 0.0268 - val_acc: 0.9880\n",
      "Epoch 58/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0540 - acc: 0.9807 - val_loss: 0.0747 - val_acc: 0.9760\n",
      "Epoch 59/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0479 - acc: 0.9872 - val_loss: 0.0296 - val_acc: 0.9940\n",
      "Epoch 60/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0398 - acc: 0.9910 - val_loss: 0.0329 - val_acc: 0.9880\n",
      "Epoch 61/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0603 - acc: 0.9807 - val_loss: 0.0407 - val_acc: 0.9820\n",
      "Epoch 62/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0399 - acc: 0.9859 - val_loss: 0.0358 - val_acc: 0.9880\n",
      "Epoch 63/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0671 - acc: 0.9795 - val_loss: 0.0347 - val_acc: 0.9940\n",
      "Epoch 64/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0526 - acc: 0.9820 - val_loss: 0.0343 - val_acc: 0.9880\n",
      "Epoch 65/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0316 - acc: 0.9859 - val_loss: 0.0359 - val_acc: 0.9880\n",
      "Epoch 66/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0259 - acc: 0.9884 - val_loss: 0.0708 - val_acc: 0.9820\n",
      "Epoch 67/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0588 - acc: 0.9820 - val_loss: 0.0350 - val_acc: 0.9880\n",
      "Epoch 68/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0241 - acc: 0.9936 - val_loss: 0.0554 - val_acc: 0.9940\n",
      "Epoch 69/200\n",
      "779/779 [==============================] - 0s 45us/step - loss: 0.0347 - acc: 0.9910 - val_loss: 0.0704 - val_acc: 0.9760\n",
      "Epoch 70/200\n",
      "779/779 [==============================] - 0s 43us/step - loss: 0.0806 - acc: 0.9820 - val_loss: 0.0468 - val_acc: 0.9820\n",
      "Epoch 71/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0247 - acc: 0.9923 - val_loss: 0.0533 - val_acc: 0.9820\n",
      "Epoch 72/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0363 - acc: 0.9897 - val_loss: 0.0412 - val_acc: 0.9880\n",
      "Epoch 73/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0282 - acc: 0.9897 - val_loss: 0.0904 - val_acc: 0.9820\n",
      "Epoch 74/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0352 - acc: 0.9897 - val_loss: 0.1188 - val_acc: 0.9760\n",
      "Epoch 75/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0430 - acc: 0.9859 - val_loss: 0.0472 - val_acc: 0.9880\n",
      "Epoch 76/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0173 - acc: 0.9961 - val_loss: 0.1306 - val_acc: 0.9760\n",
      "Epoch 77/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0514 - acc: 0.9846 - val_loss: 0.0370 - val_acc: 0.9880\n",
      "Epoch 78/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0677 - acc: 0.9807 - val_loss: 0.0310 - val_acc: 0.9880\n",
      "Epoch 79/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0281 - acc: 0.9897 - val_loss: 0.0333 - val_acc: 0.9880\n",
      "Epoch 80/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0386 - acc: 0.9859 - val_loss: 0.0667 - val_acc: 0.9820\n",
      "Epoch 81/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0138 - acc: 0.9949 - val_loss: 0.0512 - val_acc: 0.9880\n",
      "Epoch 82/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0358 - acc: 0.9923 - val_loss: 0.0430 - val_acc: 0.9940\n",
      "Epoch 83/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0303 - acc: 0.9897 - val_loss: 0.0664 - val_acc: 0.9820\n",
      "Epoch 84/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0149 - acc: 0.9961 - val_loss: 0.0827 - val_acc: 0.9760\n",
      "Epoch 85/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0454 - acc: 0.9859 - val_loss: 0.0420 - val_acc: 0.9880\n",
      "Epoch 86/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0273 - acc: 0.9897 - val_loss: 0.0782 - val_acc: 0.9820\n",
      "Epoch 87/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0258 - acc: 0.9910 - val_loss: 0.0550 - val_acc: 0.9880\n",
      "Epoch 88/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0284 - acc: 0.9910 - val_loss: 0.0830 - val_acc: 0.9701\n",
      "Epoch 89/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0409 - acc: 0.9859 - val_loss: 0.1563 - val_acc: 0.9581\n",
      "Epoch 90/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0405 - acc: 0.9872 - val_loss: 0.0637 - val_acc: 0.9820\n",
      "Epoch 91/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0415 - acc: 0.9820 - val_loss: 0.0444 - val_acc: 0.9880\n",
      "Epoch 92/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0215 - acc: 0.9897 - val_loss: 0.0805 - val_acc: 0.9820\n",
      "Epoch 93/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0256 - acc: 0.9884 - val_loss: 0.1932 - val_acc: 0.9701\n",
      "Epoch 94/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0296 - acc: 0.9923 - val_loss: 0.1278 - val_acc: 0.9760\n",
      "Epoch 95/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0151 - acc: 0.9961 - val_loss: 0.0806 - val_acc: 0.9820\n",
      "Epoch 96/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0378 - acc: 0.9897 - val_loss: 0.1039 - val_acc: 0.9760\n",
      "Epoch 97/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0355 - acc: 0.9884 - val_loss: 0.0578 - val_acc: 0.9880\n",
      "Epoch 98/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0339 - acc: 0.9846 - val_loss: 0.0551 - val_acc: 0.9880\n",
      "Epoch 99/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0285 - acc: 0.9897 - val_loss: 0.0676 - val_acc: 0.9880\n",
      "Epoch 100/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0601 - acc: 0.9795 - val_loss: 0.0561 - val_acc: 0.9880\n",
      "Epoch 101/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0421 - acc: 0.9897 - val_loss: 0.1124 - val_acc: 0.9701\n",
      "Epoch 102/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0098 - acc: 0.9974 - val_loss: 0.0763 - val_acc: 0.9820\n",
      "Epoch 103/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0165 - acc: 0.9936 - val_loss: 0.0787 - val_acc: 0.9820\n",
      "Epoch 104/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0247 - acc: 0.9923 - val_loss: 0.0847 - val_acc: 0.9820\n",
      "Epoch 105/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0308 - acc: 0.9897 - val_loss: 0.0869 - val_acc: 0.9880\n",
      "Epoch 106/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0213 - acc: 0.9936 - val_loss: 0.0828 - val_acc: 0.9820\n",
      "Epoch 107/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0414 - acc: 0.9923 - val_loss: 0.1474 - val_acc: 0.9641\n",
      "Epoch 108/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0171 - acc: 0.9910 - val_loss: 0.1461 - val_acc: 0.9760\n",
      "Epoch 109/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0450 - acc: 0.9872 - val_loss: 0.0477 - val_acc: 0.9820\n",
      "Epoch 110/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0270 - acc: 0.9910 - val_loss: 0.2129 - val_acc: 0.9521\n",
      "Epoch 111/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0288 - acc: 0.9910 - val_loss: 0.0750 - val_acc: 0.9820\n",
      "Epoch 112/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0484 - acc: 0.9846 - val_loss: 0.0443 - val_acc: 0.9820\n",
      "Epoch 113/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0332 - acc: 0.9859 - val_loss: 0.0427 - val_acc: 0.9820\n",
      "Epoch 114/200\n",
      "779/779 [==============================] - 0s 41us/step - loss: 0.0281 - acc: 0.9910 - val_loss: 0.0446 - val_acc: 0.9880\n",
      "Epoch 115/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0192 - acc: 0.9923 - val_loss: 0.1234 - val_acc: 0.9701\n",
      "Epoch 116/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0264 - acc: 0.9949 - val_loss: 0.0514 - val_acc: 0.9880\n",
      "Epoch 117/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0103 - acc: 0.9961 - val_loss: 0.0628 - val_acc: 0.9880\n",
      "Epoch 118/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0558 - acc: 0.9897 - val_loss: 0.0839 - val_acc: 0.9820\n",
      "Epoch 119/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0216 - acc: 0.9910 - val_loss: 0.0980 - val_acc: 0.9701\n",
      "Epoch 120/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0057 - acc: 0.9974 - val_loss: 0.0781 - val_acc: 0.9820\n",
      "Epoch 121/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0287 - acc: 0.9910 - val_loss: 0.0939 - val_acc: 0.9820\n",
      "Epoch 122/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0692 - acc: 0.9846 - val_loss: 0.0836 - val_acc: 0.9760\n",
      "Epoch 123/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0586 - acc: 0.9846 - val_loss: 0.0652 - val_acc: 0.9820\n",
      "Epoch 124/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0122 - acc: 0.9949 - val_loss: 0.1010 - val_acc: 0.9820\n",
      "Epoch 125/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0065 - acc: 0.9974 - val_loss: 0.0983 - val_acc: 0.9820\n",
      "Epoch 126/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0286 - acc: 0.9949 - val_loss: 0.0947 - val_acc: 0.9880\n",
      "Epoch 127/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0165 - acc: 0.9961 - val_loss: 0.1033 - val_acc: 0.9940\n",
      "Epoch 128/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0173 - acc: 0.9936 - val_loss: 0.0921 - val_acc: 0.9880\n",
      "Epoch 129/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0395 - acc: 0.9910 - val_loss: 0.1888 - val_acc: 0.9760\n",
      "Epoch 130/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0169 - acc: 0.9910 - val_loss: 0.0830 - val_acc: 0.9880\n",
      "Epoch 131/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0069 - acc: 0.9961 - val_loss: 0.0644 - val_acc: 0.9880\n",
      "Epoch 132/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0464 - acc: 0.9897 - val_loss: 0.2007 - val_acc: 0.9641\n",
      "Epoch 133/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0205 - acc: 0.9923 - val_loss: 0.1369 - val_acc: 0.9820\n",
      "Epoch 134/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0224 - acc: 0.9923 - val_loss: 0.1100 - val_acc: 0.9820\n",
      "Epoch 135/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0231 - acc: 0.9936 - val_loss: 0.0787 - val_acc: 0.9820\n",
      "Epoch 136/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0214 - acc: 0.9974 - val_loss: 0.0954 - val_acc: 0.9880\n",
      "Epoch 137/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0093 - acc: 0.9974 - val_loss: 0.0792 - val_acc: 0.9880\n",
      "Epoch 138/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0203 - acc: 0.9910 - val_loss: 0.1261 - val_acc: 0.9880\n",
      "Epoch 139/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0467 - acc: 0.9859 - val_loss: 0.1268 - val_acc: 0.9820\n",
      "Epoch 140/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0162 - acc: 0.9910 - val_loss: 0.1249 - val_acc: 0.9820\n",
      "Epoch 141/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0058 - acc: 0.9987 - val_loss: 0.0751 - val_acc: 0.9880\n",
      "Epoch 142/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.1045 - val_acc: 0.9880\n",
      "Epoch 143/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0394 - acc: 0.9923 - val_loss: 0.0738 - val_acc: 0.9880\n",
      "Epoch 144/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0510 - acc: 0.9872 - val_loss: 0.0967 - val_acc: 0.9880\n",
      "Epoch 145/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0072 - acc: 0.9987 - val_loss: 0.1210 - val_acc: 0.9760\n",
      "Epoch 146/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0057 - acc: 0.9974 - val_loss: 0.1000 - val_acc: 0.9760\n",
      "Epoch 147/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0161 - acc: 0.9987 - val_loss: 0.1164 - val_acc: 0.9701\n",
      "Epoch 148/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0288 - acc: 0.9936 - val_loss: 0.1363 - val_acc: 0.9820\n",
      "Epoch 149/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0455 - acc: 0.9897 - val_loss: 0.0930 - val_acc: 0.9880\n",
      "Epoch 150/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0381 - acc: 0.9872 - val_loss: 0.1284 - val_acc: 0.9820\n",
      "Epoch 151/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0049 - acc: 0.9974 - val_loss: 0.0977 - val_acc: 0.9820\n",
      "Epoch 152/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0186 - acc: 0.9949 - val_loss: 0.0829 - val_acc: 0.9880\n",
      "Epoch 153/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0356 - acc: 0.9872 - val_loss: 0.0716 - val_acc: 0.9940\n",
      "Epoch 154/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0071 - acc: 0.9961 - val_loss: 0.1053 - val_acc: 0.9820\n",
      "Epoch 155/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0123 - acc: 0.9961 - val_loss: 0.1041 - val_acc: 0.9880\n",
      "Epoch 156/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0419 - acc: 0.9910 - val_loss: 0.1068 - val_acc: 0.9820\n",
      "Epoch 157/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0053 - acc: 0.9974 - val_loss: 0.1078 - val_acc: 0.9880\n",
      "Epoch 158/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0173 - acc: 0.9961 - val_loss: 0.1481 - val_acc: 0.9760\n",
      "Epoch 159/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0333 - acc: 0.9923 - val_loss: 0.0814 - val_acc: 0.9880\n",
      "Epoch 160/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0090 - acc: 0.9987 - val_loss: 0.1105 - val_acc: 0.9880\n",
      "Epoch 161/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0077 - acc: 0.9974 - val_loss: 0.1031 - val_acc: 0.9880\n",
      "Epoch 162/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0117 - acc: 0.9974 - val_loss: 0.1846 - val_acc: 0.9760\n",
      "Epoch 163/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0190 - acc: 0.9961 - val_loss: 0.1452 - val_acc: 0.9820\n",
      "Epoch 164/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0402 - acc: 0.9923 - val_loss: 0.1387 - val_acc: 0.9880\n",
      "Epoch 165/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0138 - acc: 0.9961 - val_loss: 0.1374 - val_acc: 0.9820\n",
      "Epoch 166/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0249 - acc: 0.9949 - val_loss: 0.0961 - val_acc: 0.9940\n",
      "Epoch 167/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0324 - acc: 0.9910 - val_loss: 0.1034 - val_acc: 0.9880\n",
      "Epoch 168/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0092 - acc: 0.9974 - val_loss: 0.1027 - val_acc: 0.9880\n",
      "Epoch 169/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0024 - acc: 0.9987 - val_loss: 0.1201 - val_acc: 0.9880\n",
      "Epoch 170/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 5.7230e-04 - acc: 1.0000 - val_loss: 0.1214 - val_acc: 0.9880\n",
      "Epoch 171/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0167 - acc: 0.9936 - val_loss: 0.1280 - val_acc: 0.9820\n",
      "Epoch 172/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0458 - acc: 0.9936 - val_loss: 0.0991 - val_acc: 0.9940\n",
      "Epoch 173/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0378 - acc: 0.9936 - val_loss: 0.1565 - val_acc: 0.9760\n",
      "Epoch 174/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0020 - acc: 0.9987 - val_loss: 0.1288 - val_acc: 0.9820\n",
      "Epoch 175/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0160 - acc: 0.9949 - val_loss: 0.1425 - val_acc: 0.9820\n",
      "Epoch 176/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0105 - acc: 0.9961 - val_loss: 0.1189 - val_acc: 0.9820\n",
      "Epoch 177/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0483 - acc: 0.9897 - val_loss: 0.1820 - val_acc: 0.9701\n",
      "Epoch 178/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0153 - acc: 0.9987 - val_loss: 0.1458 - val_acc: 0.9760\n",
      "Epoch 179/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0284 - acc: 0.9897 - val_loss: 0.1098 - val_acc: 0.9880\n",
      "Epoch 180/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.1887 - val_acc: 0.9820\n",
      "Epoch 181/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0165 - acc: 0.9936 - val_loss: 0.1012 - val_acc: 0.9880\n",
      "Epoch 182/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0084 - acc: 0.9987 - val_loss: 0.1049 - val_acc: 0.9880\n",
      "Epoch 183/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0163 - acc: 0.9936 - val_loss: 0.0995 - val_acc: 0.9940\n",
      "Epoch 184/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0846 - acc: 0.9859 - val_loss: 0.0984 - val_acc: 0.9940\n",
      "Epoch 185/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0525 - acc: 0.9859 - val_loss: 0.1153 - val_acc: 0.9760\n",
      "Epoch 186/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0059 - acc: 0.9974 - val_loss: 0.1017 - val_acc: 0.9880\n",
      "Epoch 187/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0079 - acc: 0.9949 - val_loss: 0.1330 - val_acc: 0.9820\n",
      "Epoch 188/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0070 - acc: 0.9974 - val_loss: 0.1728 - val_acc: 0.9760\n",
      "Epoch 189/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0623 - acc: 0.9884 - val_loss: 0.0985 - val_acc: 0.9880\n",
      "Epoch 190/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0026 - acc: 0.9987 - val_loss: 0.1040 - val_acc: 0.9880\n",
      "Epoch 191/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0231 - acc: 0.9949 - val_loss: 0.1681 - val_acc: 0.9701\n",
      "Epoch 192/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0072 - acc: 0.9961 - val_loss: 0.1363 - val_acc: 0.9820\n",
      "Epoch 193/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0299 - acc: 0.9923 - val_loss: 0.1148 - val_acc: 0.9880\n",
      "Epoch 194/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0083 - acc: 0.9961 - val_loss: 0.0916 - val_acc: 0.9880\n",
      "Epoch 195/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0992 - val_acc: 0.9880\n",
      "Epoch 196/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0326 - acc: 0.9897 - val_loss: 0.1189 - val_acc: 0.9880\n",
      "Epoch 197/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0132 - acc: 0.9949 - val_loss: 0.1019 - val_acc: 0.9880\n",
      "Epoch 198/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0351 - acc: 0.9897 - val_loss: 0.1140 - val_acc: 0.9820\n",
      "Epoch 199/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0300 - acc: 0.9910 - val_loss: 0.1502 - val_acc: 0.9701\n",
      "Epoch 200/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.1285 - val_acc: 0.9880\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               25088     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 35,523\n",
      "Trainable params: 35,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 779 samples, validate on 167 samples\n",
      "Epoch 1/200\n",
      "779/779 [==============================] - 0s 446us/step - loss: 0.1206 - acc: 0.9859 - val_loss: 0.0971 - val_acc: 0.9940\n",
      "Epoch 2/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0461 - acc: 0.9910 - val_loss: 0.1476 - val_acc: 0.9820\n",
      "Epoch 3/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0093 - acc: 0.9961 - val_loss: 0.2438 - val_acc: 0.9760\n",
      "Epoch 4/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0512 - acc: 0.9884 - val_loss: 0.1005 - val_acc: 0.9880\n",
      "Epoch 5/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0287 - acc: 0.9949 - val_loss: 0.1224 - val_acc: 0.9880\n",
      "Epoch 6/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0187 - acc: 0.9961 - val_loss: 0.1151 - val_acc: 0.9880\n",
      "Epoch 7/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.1098 - val_acc: 0.9880\n",
      "Epoch 8/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0476 - acc: 0.9897 - val_loss: 0.1001 - val_acc: 0.9940\n",
      "Epoch 9/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0128 - acc: 0.9949 - val_loss: 0.1176 - val_acc: 0.9880\n",
      "Epoch 10/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0730 - acc: 0.9884 - val_loss: 0.1049 - val_acc: 0.9880\n",
      "Epoch 11/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0129 - acc: 0.9949 - val_loss: 0.1085 - val_acc: 0.9880\n",
      "Epoch 12/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0242 - acc: 0.9936 - val_loss: 0.1089 - val_acc: 0.9880\n",
      "Epoch 13/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0212 - acc: 0.9961 - val_loss: 0.1003 - val_acc: 0.9880\n",
      "Epoch 14/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0050 - acc: 0.9987 - val_loss: 0.1136 - val_acc: 0.9820\n",
      "Epoch 15/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0354 - acc: 0.9936 - val_loss: 0.0977 - val_acc: 0.9880\n",
      "Epoch 16/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0179 - acc: 0.9949 - val_loss: 0.1572 - val_acc: 0.9820\n",
      "Epoch 17/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0178 - acc: 0.9949 - val_loss: 0.1360 - val_acc: 0.9820\n",
      "Epoch 18/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0351 - acc: 0.9936 - val_loss: 0.3074 - val_acc: 0.9701\n",
      "Epoch 19/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0903 - val_acc: 0.9940\n",
      "Epoch 20/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.1186 - val_acc: 0.9880\n",
      "Epoch 21/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0402 - acc: 0.9897 - val_loss: 0.1295 - val_acc: 0.9760\n",
      "Epoch 22/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0055 - acc: 0.9987 - val_loss: 0.1178 - val_acc: 0.9760\n",
      "Epoch 23/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0141 - acc: 0.9987 - val_loss: 0.1153 - val_acc: 0.9880\n",
      "Epoch 24/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.1561 - val_acc: 0.9880\n",
      "Epoch 25/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0070 - acc: 0.9974 - val_loss: 0.1595 - val_acc: 0.9820\n",
      "Epoch 26/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0282 - acc: 0.9949 - val_loss: 0.0930 - val_acc: 0.9880\n",
      "Epoch 27/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0258 - acc: 0.9949 - val_loss: 0.1401 - val_acc: 0.9820\n",
      "Epoch 28/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.0975 - val_acc: 0.9940\n",
      "Epoch 29/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 9.9265e-04 - acc: 1.0000 - val_loss: 0.1126 - val_acc: 0.9880\n",
      "Epoch 30/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.1161 - val_acc: 0.9880\n",
      "Epoch 31/200\n",
      "779/779 [==============================] - 0s 46us/step - loss: 0.0203 - acc: 0.9910 - val_loss: 0.1774 - val_acc: 0.9760\n",
      "Epoch 32/200\n",
      "779/779 [==============================] - 0s 48us/step - loss: 0.0214 - acc: 0.9949 - val_loss: 0.0900 - val_acc: 0.9820\n",
      "Epoch 33/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0333 - acc: 0.9961 - val_loss: 0.0779 - val_acc: 0.9880\n",
      "Epoch 34/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0082 - acc: 0.9961 - val_loss: 0.1133 - val_acc: 0.9880\n",
      "Epoch 35/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 9.7478e-04 - acc: 1.0000 - val_loss: 0.1472 - val_acc: 0.9820\n",
      "Epoch 36/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0358 - acc: 0.9949 - val_loss: 0.1017 - val_acc: 0.9820\n",
      "Epoch 37/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0158 - acc: 0.9910 - val_loss: 0.1149 - val_acc: 0.9820\n",
      "Epoch 38/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0475 - acc: 0.9923 - val_loss: 0.1023 - val_acc: 0.9880\n",
      "Epoch 39/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0088 - acc: 0.9949 - val_loss: 0.0842 - val_acc: 0.9880\n",
      "Epoch 40/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0165 - acc: 0.9949 - val_loss: 0.1038 - val_acc: 0.9880\n",
      "Epoch 41/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 4.6952e-04 - acc: 1.0000 - val_loss: 0.1109 - val_acc: 0.9880\n",
      "Epoch 42/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0107 - acc: 0.9974 - val_loss: 0.1473 - val_acc: 0.9820\n",
      "Epoch 43/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0227 - acc: 0.9961 - val_loss: 0.1313 - val_acc: 0.9820\n",
      "Epoch 44/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0026 - acc: 0.9987 - val_loss: 0.1009 - val_acc: 0.9940\n",
      "Epoch 45/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0156 - acc: 0.9923 - val_loss: 0.1060 - val_acc: 0.9880\n",
      "Epoch 46/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0030 - acc: 0.9987 - val_loss: 0.1862 - val_acc: 0.9760\n",
      "Epoch 47/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0027 - acc: 0.9987 - val_loss: 0.1230 - val_acc: 0.9880\n",
      "Epoch 48/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0251 - acc: 0.9936 - val_loss: 0.1331 - val_acc: 0.9880\n",
      "Epoch 49/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0081 - acc: 0.9987 - val_loss: 0.1164 - val_acc: 0.9880\n",
      "Epoch 50/200\n",
      "779/779 [==============================] - 0s 43us/step - loss: 0.0457 - acc: 0.9910 - val_loss: 0.1097 - val_acc: 0.9820\n",
      "Epoch 51/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0069 - acc: 0.9974 - val_loss: 0.0955 - val_acc: 0.9940\n",
      "Epoch 52/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0187 - acc: 0.9974 - val_loss: 0.1144 - val_acc: 0.9880\n",
      "Epoch 53/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0218 - acc: 0.9949 - val_loss: 0.1067 - val_acc: 0.9880\n",
      "Epoch 54/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0169 - acc: 0.9961 - val_loss: 0.0999 - val_acc: 0.9940\n",
      "Epoch 55/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.1195 - val_acc: 0.9820\n",
      "Epoch 56/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0058 - acc: 0.9961 - val_loss: 0.1518 - val_acc: 0.9820\n",
      "Epoch 57/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0034 - acc: 0.9987 - val_loss: 0.1357 - val_acc: 0.9880\n",
      "Epoch 58/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0590 - acc: 0.9910 - val_loss: 0.1240 - val_acc: 0.9880\n",
      "Epoch 59/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0102 - acc: 0.9961 - val_loss: 0.1248 - val_acc: 0.9880\n",
      "Epoch 60/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0068 - acc: 0.9987 - val_loss: 0.1229 - val_acc: 0.9880\n",
      "Epoch 61/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0316 - acc: 0.9936 - val_loss: 0.0969 - val_acc: 0.9880\n",
      "Epoch 62/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.1591 - val_acc: 0.9760\n",
      "Epoch 63/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0066 - acc: 0.9974 - val_loss: 0.1190 - val_acc: 0.9820\n",
      "Epoch 64/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0177 - acc: 0.9936 - val_loss: 0.0994 - val_acc: 0.9880\n",
      "Epoch 65/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0095 - acc: 0.9974 - val_loss: 0.1041 - val_acc: 0.9880\n",
      "Epoch 66/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1146 - val_acc: 0.9880\n",
      "Epoch 67/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0281 - acc: 0.9949 - val_loss: 0.1309 - val_acc: 0.9820\n",
      "Epoch 68/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0261 - acc: 0.9910 - val_loss: 0.1080 - val_acc: 0.9880\n",
      "Epoch 69/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0389 - acc: 0.9936 - val_loss: 0.1227 - val_acc: 0.9880\n",
      "Epoch 70/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0058 - acc: 0.9974 - val_loss: 0.1659 - val_acc: 0.9820\n",
      "Epoch 71/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0068 - acc: 0.9987 - val_loss: 0.1926 - val_acc: 0.9701\n",
      "Epoch 72/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0072 - acc: 0.9974 - val_loss: 0.1377 - val_acc: 0.9880\n",
      "Epoch 73/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0081 - acc: 0.9961 - val_loss: 0.1326 - val_acc: 0.9880\n",
      "Epoch 74/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0060 - acc: 0.9987 - val_loss: 0.1006 - val_acc: 0.9940\n",
      "Epoch 75/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0351 - acc: 0.9949 - val_loss: 0.1053 - val_acc: 0.9880\n",
      "Epoch 76/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.1667 - val_acc: 0.9820\n",
      "Epoch 77/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0227 - acc: 0.9961 - val_loss: 0.1389 - val_acc: 0.9820\n",
      "Epoch 78/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0320 - acc: 0.9936 - val_loss: 0.1520 - val_acc: 0.9880\n",
      "Epoch 79/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0196 - acc: 0.9949 - val_loss: 0.3412 - val_acc: 0.9641\n",
      "Epoch 80/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0288 - acc: 0.9949 - val_loss: 0.1389 - val_acc: 0.9880\n",
      "Epoch 81/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.1397 - val_acc: 0.9701\n",
      "Epoch 82/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0069 - acc: 0.9974 - val_loss: 0.1234 - val_acc: 0.9880\n",
      "Epoch 83/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0142 - acc: 0.9961 - val_loss: 0.1754 - val_acc: 0.9820\n",
      "Epoch 84/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0377 - acc: 0.9936 - val_loss: 0.1044 - val_acc: 0.9880\n",
      "Epoch 85/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0214 - acc: 0.9961 - val_loss: 0.0995 - val_acc: 0.9940\n",
      "Epoch 86/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0108 - acc: 0.9974 - val_loss: 0.1279 - val_acc: 0.9880\n",
      "Epoch 87/200\n",
      "779/779 [==============================] - 0s 46us/step - loss: 0.0089 - acc: 0.9987 - val_loss: 0.1592 - val_acc: 0.9760\n",
      "Epoch 88/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0358 - acc: 0.9910 - val_loss: 0.1187 - val_acc: 0.9880\n",
      "Epoch 89/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 9.0583e-04 - acc: 1.0000 - val_loss: 0.1145 - val_acc: 0.9880\n",
      "Epoch 90/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 5.6634e-04 - acc: 1.0000 - val_loss: 0.1154 - val_acc: 0.9880\n",
      "Epoch 91/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0193 - acc: 0.9961 - val_loss: 0.1024 - val_acc: 0.9880\n",
      "Epoch 92/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.1768 - val_acc: 0.9820\n",
      "Epoch 93/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 8.9866e-04 - acc: 1.0000 - val_loss: 0.1132 - val_acc: 0.9820\n",
      "Epoch 94/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0105 - acc: 0.9987 - val_loss: 0.1449 - val_acc: 0.9880\n",
      "Epoch 95/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0305 - acc: 0.9910 - val_loss: 0.1496 - val_acc: 0.9880\n",
      "Epoch 96/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1410 - val_acc: 0.9880\n",
      "Epoch 97/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0040 - acc: 0.9987 - val_loss: 0.1651 - val_acc: 0.9760\n",
      "Epoch 98/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0100 - acc: 0.9961 - val_loss: 0.1572 - val_acc: 0.9880\n",
      "Epoch 99/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0100 - acc: 0.9974 - val_loss: 0.1535 - val_acc: 0.9820\n",
      "Epoch 100/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0321 - acc: 0.9949 - val_loss: 0.1668 - val_acc: 0.9701\n",
      "Epoch 101/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0101 - acc: 0.9987 - val_loss: 0.1209 - val_acc: 0.9880\n",
      "Epoch 102/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0240 - acc: 0.9923 - val_loss: 0.0977 - val_acc: 0.9940\n",
      "Epoch 103/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0230 - acc: 0.9961 - val_loss: 0.1676 - val_acc: 0.9880\n",
      "Epoch 104/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0145 - acc: 0.9961 - val_loss: 0.1172 - val_acc: 0.9880\n",
      "Epoch 105/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.1684 - val_acc: 0.9880\n",
      "Epoch 106/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0026 - acc: 0.9987 - val_loss: 0.1468 - val_acc: 0.9880\n",
      "Epoch 107/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0284 - acc: 0.9936 - val_loss: 0.1727 - val_acc: 0.9820\n",
      "Epoch 108/200\n",
      "779/779 [==============================] - 0s 42us/step - loss: 0.0133 - acc: 0.9974 - val_loss: 0.1394 - val_acc: 0.9880\n",
      "Epoch 109/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0101 - acc: 0.9961 - val_loss: 0.1642 - val_acc: 0.9880\n",
      "Epoch 110/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0048 - acc: 0.9987 - val_loss: 0.1136 - val_acc: 0.9880\n",
      "Epoch 111/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0082 - acc: 0.9961 - val_loss: 0.1609 - val_acc: 0.9820\n",
      "Epoch 112/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0105 - acc: 0.9961 - val_loss: 0.1626 - val_acc: 0.9760\n",
      "Epoch 113/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0108 - acc: 0.9949 - val_loss: 0.0965 - val_acc: 0.9940\n",
      "Epoch 114/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0112 - acc: 0.9961 - val_loss: 0.1535 - val_acc: 0.9880\n",
      "Epoch 115/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0174 - acc: 0.9961 - val_loss: 0.1501 - val_acc: 0.9820\n",
      "Epoch 116/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0062 - acc: 0.9987 - val_loss: 0.1535 - val_acc: 0.9820\n",
      "Epoch 117/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 8.4696e-04 - acc: 1.0000 - val_loss: 0.1407 - val_acc: 0.9880\n",
      "Epoch 118/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0084 - acc: 0.9949 - val_loss: 0.1175 - val_acc: 0.9880\n",
      "Epoch 119/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 1.7276e-04 - acc: 1.0000 - val_loss: 0.1075 - val_acc: 0.9880\n",
      "Epoch 120/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.1114 - val_acc: 0.9820\n",
      "Epoch 121/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0271 - acc: 0.9923 - val_loss: 0.1445 - val_acc: 0.9820\n",
      "Epoch 122/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 3.6557e-04 - acc: 1.0000 - val_loss: 0.1263 - val_acc: 0.9880\n",
      "Epoch 123/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 8.3338e-04 - acc: 1.0000 - val_loss: 0.1176 - val_acc: 0.9880\n",
      "Epoch 124/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0184 - acc: 0.9961 - val_loss: 0.0971 - val_acc: 0.9940\n",
      "Epoch 125/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0309 - acc: 0.9949 - val_loss: 0.0970 - val_acc: 0.9940\n",
      "Epoch 126/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.1523 - val_acc: 0.9880\n",
      "Epoch 127/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0107 - acc: 0.9961 - val_loss: 0.1660 - val_acc: 0.9880\n",
      "Epoch 128/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 3.1982e-04 - acc: 1.0000 - val_loss: 0.1533 - val_acc: 0.9880\n",
      "Epoch 129/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 6.1477e-04 - acc: 1.0000 - val_loss: 0.1788 - val_acc: 0.9820\n",
      "Epoch 130/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0185 - acc: 0.9974 - val_loss: 0.0966 - val_acc: 0.9940\n",
      "Epoch 131/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0097 - acc: 0.9974 - val_loss: 0.0975 - val_acc: 0.9940\n",
      "Epoch 132/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0168 - acc: 0.9961 - val_loss: 0.0974 - val_acc: 0.9940\n",
      "Epoch 133/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0024 - acc: 0.9987 - val_loss: 0.0973 - val_acc: 0.9940\n",
      "Epoch 134/200\n",
      "779/779 [==============================] - 0s 48us/step - loss: 0.0434 - acc: 0.9910 - val_loss: 0.2470 - val_acc: 0.9760\n",
      "Epoch 135/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0078 - acc: 0.9987 - val_loss: 0.1471 - val_acc: 0.9880\n",
      "Epoch 136/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0021 - acc: 0.9987 - val_loss: 0.1743 - val_acc: 0.9820\n",
      "Epoch 137/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0123 - acc: 0.9961 - val_loss: 0.1379 - val_acc: 0.9880\n",
      "Epoch 138/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0300 - acc: 0.9961 - val_loss: 0.1575 - val_acc: 0.9880\n",
      "Epoch 139/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0028 - acc: 0.9987 - val_loss: 0.1629 - val_acc: 0.9880\n",
      "Epoch 140/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 4.0481e-04 - acc: 1.0000 - val_loss: 0.1590 - val_acc: 0.9880\n",
      "Epoch 141/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0018 - acc: 0.9987 - val_loss: 0.1831 - val_acc: 0.9820\n",
      "Epoch 142/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0212 - acc: 0.9987 - val_loss: 0.1004 - val_acc: 0.9940\n",
      "Epoch 143/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0312 - acc: 0.9923 - val_loss: 0.0979 - val_acc: 0.9940\n",
      "Epoch 144/200\n",
      "779/779 [==============================] - 0s 44us/step - loss: 0.0210 - acc: 0.9987 - val_loss: 0.1399 - val_acc: 0.9880\n",
      "Epoch 145/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0021 - acc: 0.9987 - val_loss: 0.1227 - val_acc: 0.9820\n",
      "Epoch 146/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0041 - acc: 0.9974 - val_loss: 0.1972 - val_acc: 0.9880\n",
      "Epoch 147/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0155 - acc: 0.9961 - val_loss: 0.1119 - val_acc: 0.9880\n",
      "Epoch 148/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 8.3215e-04 - acc: 1.0000 - val_loss: 0.1808 - val_acc: 0.9880\n",
      "Epoch 149/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 3.0622e-05 - acc: 1.0000 - val_loss: 0.1796 - val_acc: 0.9880\n",
      "Epoch 150/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0021 - acc: 0.9987 - val_loss: 0.1857 - val_acc: 0.9880\n",
      "Epoch 151/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0025 - acc: 0.9987 - val_loss: 0.2326 - val_acc: 0.9760\n",
      "Epoch 152/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0229 - acc: 0.9961 - val_loss: 0.2083 - val_acc: 0.9820\n",
      "Epoch 153/200\n",
      "779/779 [==============================] - 0s 54us/step - loss: 0.1252 - acc: 0.9872 - val_loss: 0.3678 - val_acc: 0.9641\n",
      "Epoch 154/200\n",
      "779/779 [==============================] - 0s 41us/step - loss: 0.0626 - acc: 0.9923 - val_loss: 0.0995 - val_acc: 0.9940\n",
      "Epoch 155/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0161 - acc: 0.9949 - val_loss: 0.1295 - val_acc: 0.9820\n",
      "Epoch 156/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0112 - acc: 0.9974 - val_loss: 0.1949 - val_acc: 0.9820\n",
      "Epoch 157/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0172 - acc: 0.9949 - val_loss: 0.1148 - val_acc: 0.9880\n",
      "Epoch 158/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 8.4703e-04 - acc: 1.0000 - val_loss: 0.1662 - val_acc: 0.9880\n",
      "Epoch 159/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0220 - acc: 0.9961 - val_loss: 0.2510 - val_acc: 0.9701\n",
      "Epoch 160/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0131 - acc: 0.9961 - val_loss: 0.1493 - val_acc: 0.9760\n",
      "Epoch 161/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.1732 - val_acc: 0.9880\n",
      "Epoch 162/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.1115 - val_acc: 0.9880\n",
      "Epoch 163/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 2.5533e-04 - acc: 1.0000 - val_loss: 0.1172 - val_acc: 0.9880\n",
      "Epoch 164/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2082 - val_acc: 0.9820\n",
      "Epoch 165/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0399 - acc: 0.9936 - val_loss: 0.2438 - val_acc: 0.9820\n",
      "Epoch 166/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0498 - acc: 0.9897 - val_loss: 0.1502 - val_acc: 0.9820\n",
      "Epoch 167/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0119 - acc: 0.9961 - val_loss: 0.1623 - val_acc: 0.9820\n",
      "Epoch 168/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0574 - acc: 0.9936 - val_loss: 0.1588 - val_acc: 0.9820\n",
      "Epoch 169/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0103 - acc: 0.9974 - val_loss: 0.1402 - val_acc: 0.9880\n",
      "Epoch 170/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0120 - acc: 0.9961 - val_loss: 0.1565 - val_acc: 0.9880\n",
      "Epoch 171/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0283 - acc: 0.9936 - val_loss: 0.0984 - val_acc: 0.9940\n",
      "Epoch 172/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.1301 - val_acc: 0.9880\n",
      "Epoch 173/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.1247 - val_acc: 0.9880\n",
      "Epoch 174/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 2.7856e-04 - acc: 1.0000 - val_loss: 0.1295 - val_acc: 0.9880\n",
      "Epoch 175/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0027 - acc: 0.9987 - val_loss: 0.1086 - val_acc: 0.9880\n",
      "Epoch 176/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0280 - acc: 0.9961 - val_loss: 0.1354 - val_acc: 0.9880\n",
      "Epoch 177/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0216 - acc: 0.9961 - val_loss: 0.0999 - val_acc: 0.9940\n",
      "Epoch 178/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0049 - acc: 0.9987 - val_loss: 0.1602 - val_acc: 0.9880\n",
      "Epoch 179/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0073 - acc: 0.9987 - val_loss: 0.1654 - val_acc: 0.9880\n",
      "Epoch 180/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0018 - acc: 0.9987 - val_loss: 0.1718 - val_acc: 0.9880\n",
      "Epoch 181/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0146 - acc: 0.9961 - val_loss: 0.1482 - val_acc: 0.9880\n",
      "Epoch 182/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 3.6309e-04 - acc: 1.0000 - val_loss: 0.1539 - val_acc: 0.9880\n",
      "Epoch 183/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0169 - acc: 0.9949 - val_loss: 0.1368 - val_acc: 0.9880\n",
      "Epoch 184/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0243 - acc: 0.9949 - val_loss: 0.1273 - val_acc: 0.9880\n",
      "Epoch 185/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0024 - acc: 0.9987 - val_loss: 0.1128 - val_acc: 0.9880\n",
      "Epoch 186/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0371 - acc: 0.9974 - val_loss: 0.1652 - val_acc: 0.9820\n",
      "Epoch 187/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 1.6030e-04 - acc: 1.0000 - val_loss: 0.1575 - val_acc: 0.9820\n",
      "Epoch 188/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0069 - acc: 0.9974 - val_loss: 0.1202 - val_acc: 0.9880\n",
      "Epoch 189/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0029 - acc: 0.9987 - val_loss: 0.1656 - val_acc: 0.9880\n",
      "Epoch 190/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0074 - acc: 0.9987 - val_loss: 0.1294 - val_acc: 0.9880\n",
      "Epoch 191/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.1366 - val_acc: 0.9880\n",
      "Epoch 192/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0193 - acc: 0.9949 - val_loss: 0.0965 - val_acc: 0.9940\n",
      "Epoch 193/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0189 - acc: 0.9974 - val_loss: 0.2298 - val_acc: 0.9701\n",
      "Epoch 194/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0019 - acc: 0.9987 - val_loss: 0.1540 - val_acc: 0.9880\n",
      "Epoch 195/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0164 - acc: 0.9961 - val_loss: 0.1463 - val_acc: 0.9880\n",
      "Epoch 196/200\n",
      "779/779 [==============================] - 0s 41us/step - loss: 2.4143e-04 - acc: 1.0000 - val_loss: 0.1006 - val_acc: 0.9940\n",
      "Epoch 197/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0060 - acc: 0.9974 - val_loss: 0.1669 - val_acc: 0.9820\n",
      "Epoch 198/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0017 - acc: 0.9987 - val_loss: 0.0966 - val_acc: 0.9940\n",
      "Epoch 199/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0057 - acc: 0.9974 - val_loss: 0.2063 - val_acc: 0.9820\n",
      "Epoch 200/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0023 - acc: 0.9987 - val_loss: 0.1143 - val_acc: 0.9880\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               25088     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 35,523\n",
      "Trainable params: 35,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#database = Path('C:\\\\Users\\Student\\Desktop\\Sample sounds')\n",
    "p = FFNN()\n",
    "p.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "ab6f2afe840deab5cf2b042eb9e81a867f8e56e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/librosa/core/pitch.py:145: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn('Trying to estimate tuning from empty frequency set.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 779 samples, validate on 167 samples\n",
      "Epoch 1/200\n",
      "779/779 [==============================] - 0s 535us/step - loss: 0.7609 - acc: 0.7060 - val_loss: 0.3310 - val_acc: 0.8563\n",
      "Epoch 2/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.3818 - acc: 0.8588 - val_loss: 0.1988 - val_acc: 0.9341\n",
      "Epoch 3/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.3053 - acc: 0.8858 - val_loss: 0.1122 - val_acc: 0.9641\n",
      "Epoch 4/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.2222 - acc: 0.9243 - val_loss: 0.1192 - val_acc: 0.9641\n",
      "Epoch 5/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.2245 - acc: 0.9178 - val_loss: 0.1337 - val_acc: 0.9581\n",
      "Epoch 6/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.2187 - acc: 0.9255 - val_loss: 0.0976 - val_acc: 0.9581\n",
      "Epoch 7/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1965 - acc: 0.9358 - val_loss: 0.0599 - val_acc: 0.9940\n",
      "Epoch 8/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1749 - acc: 0.9448 - val_loss: 0.0495 - val_acc: 0.9820\n",
      "Epoch 9/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.1366 - acc: 0.9602 - val_loss: 0.0400 - val_acc: 0.9940\n",
      "Epoch 10/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.1365 - acc: 0.9499 - val_loss: 0.2442 - val_acc: 0.9222\n",
      "Epoch 11/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1678 - acc: 0.9422 - val_loss: 0.0754 - val_acc: 0.9701\n",
      "Epoch 12/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.1394 - acc: 0.9538 - val_loss: 0.0225 - val_acc: 1.0000\n",
      "Epoch 13/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.1321 - acc: 0.9602 - val_loss: 0.0270 - val_acc: 1.0000\n",
      "Epoch 14/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1143 - acc: 0.9615 - val_loss: 0.0435 - val_acc: 0.9760\n",
      "Epoch 15/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1289 - acc: 0.9487 - val_loss: 0.0226 - val_acc: 0.9940\n",
      "Epoch 16/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0961 - acc: 0.9666 - val_loss: 0.0433 - val_acc: 0.9880\n",
      "Epoch 17/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1217 - acc: 0.9551 - val_loss: 0.0982 - val_acc: 0.9461\n",
      "Epoch 18/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1279 - acc: 0.9525 - val_loss: 0.0824 - val_acc: 0.9641\n",
      "Epoch 19/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1136 - acc: 0.9576 - val_loss: 0.0656 - val_acc: 0.9641\n",
      "Epoch 20/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1257 - acc: 0.9602 - val_loss: 0.0853 - val_acc: 0.9581\n",
      "Epoch 21/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0924 - acc: 0.9692 - val_loss: 0.0220 - val_acc: 0.9940\n",
      "Epoch 22/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.1039 - acc: 0.9628 - val_loss: 0.0226 - val_acc: 1.0000\n",
      "Epoch 23/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.1048 - acc: 0.9641 - val_loss: 0.0387 - val_acc: 0.9760\n",
      "Epoch 24/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.1006 - acc: 0.9641 - val_loss: 0.0830 - val_acc: 0.9581\n",
      "Epoch 25/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0904 - acc: 0.9653 - val_loss: 0.0290 - val_acc: 0.9880\n",
      "Epoch 26/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0922 - acc: 0.9641 - val_loss: 0.0257 - val_acc: 0.9880\n",
      "Epoch 27/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0854 - acc: 0.9615 - val_loss: 0.0176 - val_acc: 0.9940\n",
      "Epoch 28/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0764 - acc: 0.9730 - val_loss: 0.0243 - val_acc: 0.9880\n",
      "Epoch 29/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0718 - acc: 0.9718 - val_loss: 0.0394 - val_acc: 0.9880\n",
      "Epoch 30/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0787 - acc: 0.9730 - val_loss: 0.0360 - val_acc: 0.9880\n",
      "Epoch 31/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0914 - acc: 0.9641 - val_loss: 0.0366 - val_acc: 0.9820\n",
      "Epoch 32/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0674 - acc: 0.9718 - val_loss: 0.0905 - val_acc: 0.9701\n",
      "Epoch 33/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0770 - acc: 0.9769 - val_loss: 0.0265 - val_acc: 0.9940\n",
      "Epoch 34/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0548 - acc: 0.9769 - val_loss: 0.0159 - val_acc: 0.9880\n",
      "Epoch 35/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0757 - acc: 0.9743 - val_loss: 0.0389 - val_acc: 0.9820\n",
      "Epoch 36/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0633 - acc: 0.9795 - val_loss: 0.0393 - val_acc: 0.9880\n",
      "Epoch 37/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0684 - acc: 0.9743 - val_loss: 0.0225 - val_acc: 0.9880\n",
      "Epoch 38/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0784 - acc: 0.9756 - val_loss: 0.0711 - val_acc: 0.9760\n",
      "Epoch 39/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0527 - acc: 0.9807 - val_loss: 0.0250 - val_acc: 0.9880\n",
      "Epoch 40/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0520 - acc: 0.9769 - val_loss: 0.0244 - val_acc: 0.9820\n",
      "Epoch 41/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0689 - acc: 0.9730 - val_loss: 0.0148 - val_acc: 0.9940\n",
      "Epoch 42/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0712 - acc: 0.9769 - val_loss: 0.0237 - val_acc: 0.9940\n",
      "Epoch 43/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0626 - acc: 0.9820 - val_loss: 0.0236 - val_acc: 0.9880\n",
      "Epoch 44/200\n",
      "779/779 [==============================] - 0s 42us/step - loss: 0.0431 - acc: 0.9833 - val_loss: 0.2065 - val_acc: 0.9281\n",
      "Epoch 45/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0624 - acc: 0.9782 - val_loss: 0.0312 - val_acc: 0.9820\n",
      "Epoch 46/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0370 - acc: 0.9859 - val_loss: 0.0184 - val_acc: 0.9940\n",
      "Epoch 47/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0559 - acc: 0.9795 - val_loss: 0.0487 - val_acc: 0.9701\n",
      "Epoch 48/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0643 - acc: 0.9807 - val_loss: 0.0233 - val_acc: 0.9820\n",
      "Epoch 49/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0479 - acc: 0.9807 - val_loss: 0.0304 - val_acc: 0.9880\n",
      "Epoch 50/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0560 - acc: 0.9795 - val_loss: 0.1514 - val_acc: 0.9461\n",
      "Epoch 51/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0693 - acc: 0.9769 - val_loss: 0.0728 - val_acc: 0.9760\n",
      "Epoch 52/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0483 - acc: 0.9846 - val_loss: 0.0245 - val_acc: 0.9880\n",
      "Epoch 53/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0320 - acc: 0.9884 - val_loss: 0.0295 - val_acc: 0.9880\n",
      "Epoch 54/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0874 - acc: 0.9743 - val_loss: 0.0457 - val_acc: 0.9760\n",
      "Epoch 55/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0262 - acc: 0.9923 - val_loss: 0.0648 - val_acc: 0.9820\n",
      "Epoch 56/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0820 - acc: 0.9718 - val_loss: 0.0202 - val_acc: 0.9880\n",
      "Epoch 57/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0414 - acc: 0.9833 - val_loss: 0.0238 - val_acc: 0.9880\n",
      "Epoch 58/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0482 - acc: 0.9807 - val_loss: 0.0545 - val_acc: 0.9760\n",
      "Epoch 59/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0430 - acc: 0.9872 - val_loss: 0.0261 - val_acc: 0.9880\n",
      "Epoch 60/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0755 - acc: 0.9833 - val_loss: 0.0311 - val_acc: 0.9820\n",
      "Epoch 61/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0557 - acc: 0.9833 - val_loss: 0.0518 - val_acc: 0.9760\n",
      "Epoch 62/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0295 - acc: 0.9859 - val_loss: 0.0213 - val_acc: 0.9940\n",
      "Epoch 63/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0430 - acc: 0.9872 - val_loss: 0.0197 - val_acc: 0.9880\n",
      "Epoch 64/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0440 - acc: 0.9833 - val_loss: 0.0226 - val_acc: 0.9880\n",
      "Epoch 65/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0413 - acc: 0.9846 - val_loss: 0.0255 - val_acc: 0.9880\n",
      "Epoch 66/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0341 - acc: 0.9846 - val_loss: 0.0287 - val_acc: 0.9940\n",
      "Epoch 67/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0597 - acc: 0.9743 - val_loss: 0.1394 - val_acc: 0.9641\n",
      "Epoch 68/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0388 - acc: 0.9884 - val_loss: 0.0315 - val_acc: 0.9880\n",
      "Epoch 69/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0116 - acc: 0.9974 - val_loss: 0.0418 - val_acc: 0.9880\n",
      "Epoch 70/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0688 - acc: 0.9807 - val_loss: 0.0796 - val_acc: 0.9760\n",
      "Epoch 71/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0298 - acc: 0.9884 - val_loss: 0.0483 - val_acc: 0.9820\n",
      "Epoch 72/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0296 - acc: 0.9872 - val_loss: 0.0526 - val_acc: 0.9760\n",
      "Epoch 73/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0318 - acc: 0.9872 - val_loss: 0.0413 - val_acc: 0.9880\n",
      "Epoch 74/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0338 - acc: 0.9872 - val_loss: 0.0415 - val_acc: 0.9880\n",
      "Epoch 75/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0484 - acc: 0.9820 - val_loss: 0.0432 - val_acc: 0.9880\n",
      "Epoch 76/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0186 - acc: 0.9949 - val_loss: 0.1060 - val_acc: 0.9760\n",
      "Epoch 77/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0267 - acc: 0.9872 - val_loss: 0.0304 - val_acc: 0.9880\n",
      "Epoch 78/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0564 - acc: 0.9859 - val_loss: 0.0424 - val_acc: 0.9820\n",
      "Epoch 79/200\n",
      "779/779 [==============================] - 0s 35us/step - loss: 0.0379 - acc: 0.9923 - val_loss: 0.0391 - val_acc: 0.9880\n",
      "Epoch 80/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0400 - acc: 0.9846 - val_loss: 0.0620 - val_acc: 0.9880\n",
      "Epoch 81/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0402 - acc: 0.9859 - val_loss: 0.1640 - val_acc: 0.9701\n",
      "Epoch 82/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0376 - acc: 0.9872 - val_loss: 0.0423 - val_acc: 0.9880\n",
      "Epoch 83/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0303 - acc: 0.9910 - val_loss: 0.0407 - val_acc: 0.9880\n",
      "Epoch 84/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0339 - acc: 0.9872 - val_loss: 0.0611 - val_acc: 0.9880\n",
      "Epoch 85/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0313 - acc: 0.9859 - val_loss: 0.0503 - val_acc: 0.9880\n",
      "Epoch 86/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0201 - acc: 0.9923 - val_loss: 0.0563 - val_acc: 0.9880\n",
      "Epoch 87/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0290 - acc: 0.9884 - val_loss: 0.0421 - val_acc: 0.9880\n",
      "Epoch 88/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0180 - acc: 0.9936 - val_loss: 0.0960 - val_acc: 0.9820\n",
      "Epoch 89/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0478 - acc: 0.9859 - val_loss: 0.0886 - val_acc: 0.9820\n",
      "Epoch 90/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0357 - acc: 0.9884 - val_loss: 0.0460 - val_acc: 0.9880\n",
      "Epoch 91/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0336 - acc: 0.9897 - val_loss: 0.0409 - val_acc: 0.9880\n",
      "Epoch 92/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0434 - acc: 0.9846 - val_loss: 0.0505 - val_acc: 0.9820\n",
      "Epoch 93/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0357 - acc: 0.9833 - val_loss: 0.0552 - val_acc: 0.9820\n",
      "Epoch 94/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0322 - acc: 0.9910 - val_loss: 0.0493 - val_acc: 0.9820\n",
      "Epoch 95/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0205 - acc: 0.9961 - val_loss: 0.0588 - val_acc: 0.9820\n",
      "Epoch 96/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0323 - acc: 0.9897 - val_loss: 0.0554 - val_acc: 0.9820\n",
      "Epoch 97/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0300 - acc: 0.9897 - val_loss: 0.0650 - val_acc: 0.9820\n",
      "Epoch 98/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0357 - acc: 0.9846 - val_loss: 0.0577 - val_acc: 0.9820\n",
      "Epoch 99/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0213 - acc: 0.9923 - val_loss: 0.0683 - val_acc: 0.9880\n",
      "Epoch 100/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0506 - acc: 0.9820 - val_loss: 0.0525 - val_acc: 0.9820\n",
      "Epoch 101/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0365 - acc: 0.9936 - val_loss: 0.0488 - val_acc: 0.9880\n",
      "Epoch 102/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.0849 - val_acc: 0.9880\n",
      "Epoch 103/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0366 - acc: 0.9910 - val_loss: 0.0548 - val_acc: 0.9940\n",
      "Epoch 104/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0336 - acc: 0.9923 - val_loss: 0.0675 - val_acc: 0.9880\n",
      "Epoch 105/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0302 - acc: 0.9923 - val_loss: 0.0544 - val_acc: 0.9880\n",
      "Epoch 106/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0278 - acc: 0.9897 - val_loss: 0.0485 - val_acc: 0.9940\n",
      "Epoch 107/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0091 - acc: 0.9974 - val_loss: 0.0723 - val_acc: 0.9820\n",
      "Epoch 108/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0255 - acc: 0.9897 - val_loss: 0.0510 - val_acc: 0.9880\n",
      "Epoch 109/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0239 - acc: 0.9910 - val_loss: 0.0807 - val_acc: 0.9820\n",
      "Epoch 110/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0293 - acc: 0.9897 - val_loss: 0.1291 - val_acc: 0.9641\n",
      "Epoch 111/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0690 - acc: 0.9872 - val_loss: 0.0438 - val_acc: 0.9940\n",
      "Epoch 112/200\n",
      "779/779 [==============================] - 0s 51us/step - loss: 0.0232 - acc: 0.9936 - val_loss: 0.0475 - val_acc: 0.9880\n",
      "Epoch 113/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0366 - acc: 0.9910 - val_loss: 0.0493 - val_acc: 0.9820\n",
      "Epoch 114/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0264 - acc: 0.9910 - val_loss: 0.0461 - val_acc: 0.9880\n",
      "Epoch 115/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0472 - val_acc: 0.9880\n",
      "Epoch 116/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0217 - acc: 0.9923 - val_loss: 0.0403 - val_acc: 0.9880\n",
      "Epoch 117/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0165 - acc: 0.9923 - val_loss: 0.0457 - val_acc: 0.9880\n",
      "Epoch 118/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0238 - acc: 0.9936 - val_loss: 0.0833 - val_acc: 0.9820\n",
      "Epoch 119/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0209 - acc: 0.9936 - val_loss: 0.0491 - val_acc: 0.9880\n",
      "Epoch 120/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0109 - acc: 0.9936 - val_loss: 0.0630 - val_acc: 0.9820\n",
      "Epoch 121/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0383 - acc: 0.9910 - val_loss: 0.1313 - val_acc: 0.9760\n",
      "Epoch 122/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0463 - acc: 0.9872 - val_loss: 0.0483 - val_acc: 0.9880\n",
      "Epoch 123/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0534 - acc: 0.9884 - val_loss: 0.0576 - val_acc: 0.9820\n",
      "Epoch 124/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0197 - acc: 0.9910 - val_loss: 0.0802 - val_acc: 0.9820\n",
      "Epoch 125/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0276 - acc: 0.9936 - val_loss: 0.0560 - val_acc: 0.9880\n",
      "Epoch 126/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0239 - acc: 0.9923 - val_loss: 0.0861 - val_acc: 0.9880\n",
      "Epoch 127/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0359 - acc: 0.9936 - val_loss: 0.0661 - val_acc: 0.9940\n",
      "Epoch 128/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0043 - acc: 0.9961 - val_loss: 0.1054 - val_acc: 0.9820\n",
      "Epoch 129/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0144 - acc: 0.9974 - val_loss: 0.0766 - val_acc: 0.9880\n",
      "Epoch 130/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0098 - acc: 0.9949 - val_loss: 0.1303 - val_acc: 0.9820\n",
      "Epoch 131/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0287 - acc: 0.9897 - val_loss: 0.1680 - val_acc: 0.9701\n",
      "Epoch 132/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0075 - acc: 0.9974 - val_loss: 0.0986 - val_acc: 0.9880\n",
      "Epoch 133/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0303 - acc: 0.9936 - val_loss: 0.0880 - val_acc: 0.9880\n",
      "Epoch 134/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0200 - acc: 0.9949 - val_loss: 0.0989 - val_acc: 0.9880\n",
      "Epoch 135/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0240 - acc: 0.9910 - val_loss: 0.0965 - val_acc: 0.9880\n",
      "Epoch 136/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0051 - acc: 0.9987 - val_loss: 0.1164 - val_acc: 0.9880\n",
      "Epoch 137/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0048 - acc: 0.9974 - val_loss: 0.1612 - val_acc: 0.9880\n",
      "Epoch 138/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0280 - acc: 0.9923 - val_loss: 0.0925 - val_acc: 0.9880\n",
      "Epoch 139/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0206 - acc: 0.9910 - val_loss: 0.2339 - val_acc: 0.9641\n",
      "Epoch 140/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0325 - acc: 0.9910 - val_loss: 0.0535 - val_acc: 0.9880\n",
      "Epoch 141/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0119 - acc: 0.9974 - val_loss: 0.1110 - val_acc: 0.9760\n",
      "Epoch 142/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0698 - acc: 0.9833 - val_loss: 0.0649 - val_acc: 0.9880\n",
      "Epoch 143/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0301 - acc: 0.9859 - val_loss: 0.0507 - val_acc: 0.9880\n",
      "Epoch 144/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0119 - acc: 0.9974 - val_loss: 0.0656 - val_acc: 0.9940\n",
      "Epoch 145/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0442 - acc: 0.9923 - val_loss: 0.0676 - val_acc: 0.9880\n",
      "Epoch 146/200\n",
      "779/779 [==============================] - 0s 48us/step - loss: 0.0195 - acc: 0.9936 - val_loss: 0.0686 - val_acc: 0.9880\n",
      "Epoch 147/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0638 - val_acc: 0.9940\n",
      "Epoch 148/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0758 - acc: 0.9846 - val_loss: 0.0700 - val_acc: 0.9760\n",
      "Epoch 149/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0072 - acc: 0.9974 - val_loss: 0.1161 - val_acc: 0.9760\n",
      "Epoch 150/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0200 - acc: 0.9936 - val_loss: 0.0734 - val_acc: 0.9880\n",
      "Epoch 151/200\n",
      "779/779 [==============================] - 0s 50us/step - loss: 0.0109 - acc: 0.9974 - val_loss: 0.0729 - val_acc: 0.9820\n",
      "Epoch 152/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0037 - acc: 0.9974 - val_loss: 0.1014 - val_acc: 0.9820\n",
      "Epoch 153/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0538 - acc: 0.9820 - val_loss: 0.1234 - val_acc: 0.9701\n",
      "Epoch 154/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0062 - acc: 0.9974 - val_loss: 0.0734 - val_acc: 0.9880\n",
      "Epoch 155/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0072 - acc: 0.9974 - val_loss: 0.0640 - val_acc: 0.9880\n",
      "Epoch 156/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0118 - acc: 0.9974 - val_loss: 0.0763 - val_acc: 0.9880\n",
      "Epoch 157/200\n",
      "779/779 [==============================] - 0s 45us/step - loss: 0.0664 - acc: 0.9859 - val_loss: 0.0725 - val_acc: 0.9880\n",
      "Epoch 158/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0381 - acc: 0.9949 - val_loss: 0.0622 - val_acc: 0.9880\n",
      "Epoch 159/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0060 - acc: 0.9961 - val_loss: 0.0776 - val_acc: 0.9880\n",
      "Epoch 160/200\n",
      "779/779 [==============================] - 0s 48us/step - loss: 0.0231 - acc: 0.9923 - val_loss: 0.1062 - val_acc: 0.9880\n",
      "Epoch 161/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0186 - acc: 0.9949 - val_loss: 0.0839 - val_acc: 0.9880\n",
      "Epoch 162/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.1015 - val_acc: 0.9880\n",
      "Epoch 163/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0190 - acc: 0.9910 - val_loss: 0.0683 - val_acc: 0.9940\n",
      "Epoch 164/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.0981 - val_acc: 0.9880\n",
      "Epoch 165/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0288 - acc: 0.9936 - val_loss: 0.0942 - val_acc: 0.9820\n",
      "Epoch 166/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0269 - acc: 0.9936 - val_loss: 0.0932 - val_acc: 0.9880\n",
      "Epoch 167/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.1045 - val_acc: 0.9880\n",
      "Epoch 168/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0151 - acc: 0.9961 - val_loss: 0.0844 - val_acc: 0.9940\n",
      "Epoch 169/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0041 - acc: 0.9974 - val_loss: 0.0920 - val_acc: 0.9940\n",
      "Epoch 170/200\n",
      "779/779 [==============================] - 0s 41us/step - loss: 0.0527 - acc: 0.9884 - val_loss: 0.0821 - val_acc: 0.9880\n",
      "Epoch 171/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0056 - acc: 0.9987 - val_loss: 0.0850 - val_acc: 0.9880\n",
      "Epoch 172/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0059 - acc: 0.9961 - val_loss: 0.0794 - val_acc: 0.9880\n",
      "Epoch 173/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0210 - acc: 0.9923 - val_loss: 0.0868 - val_acc: 0.9940\n",
      "Epoch 174/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0327 - acc: 0.9936 - val_loss: 0.0904 - val_acc: 0.9820\n",
      "Epoch 175/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0410 - acc: 0.9936 - val_loss: 0.1082 - val_acc: 0.9820\n",
      "Epoch 176/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0202 - acc: 0.9936 - val_loss: 0.0767 - val_acc: 0.9880\n",
      "Epoch 177/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0154 - acc: 0.9949 - val_loss: 0.0849 - val_acc: 0.9880\n",
      "Epoch 178/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0203 - acc: 0.9936 - val_loss: 0.0851 - val_acc: 0.9820\n",
      "Epoch 179/200\n",
      "779/779 [==============================] - 0s 36us/step - loss: 0.0189 - acc: 0.9949 - val_loss: 0.0764 - val_acc: 0.9940\n",
      "Epoch 180/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0272 - acc: 0.9910 - val_loss: 0.1012 - val_acc: 0.9880\n",
      "Epoch 181/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0086 - acc: 0.9974 - val_loss: 0.0964 - val_acc: 0.9880\n",
      "Epoch 182/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0385 - acc: 0.9936 - val_loss: 0.0795 - val_acc: 0.9880\n",
      "Epoch 183/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0154 - acc: 0.9961 - val_loss: 0.0954 - val_acc: 0.9880\n",
      "Epoch 184/200\n",
      "779/779 [==============================] - 0s 39us/step - loss: 0.0099 - acc: 0.9961 - val_loss: 0.1026 - val_acc: 0.9880\n",
      "Epoch 185/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0157 - acc: 0.9961 - val_loss: 0.1747 - val_acc: 0.9701\n",
      "Epoch 186/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0242 - acc: 0.9923 - val_loss: 0.1173 - val_acc: 0.9701\n",
      "Epoch 187/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0207 - acc: 0.9961 - val_loss: 0.0924 - val_acc: 0.9880\n",
      "Epoch 188/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0271 - acc: 0.9949 - val_loss: 0.0761 - val_acc: 0.9940\n",
      "Epoch 189/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0061 - acc: 0.9974 - val_loss: 0.0797 - val_acc: 0.9880\n",
      "Epoch 190/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0117 - acc: 0.9949 - val_loss: 0.0865 - val_acc: 0.9940\n",
      "Epoch 191/200\n",
      "779/779 [==============================] - 0s 41us/step - loss: 0.0159 - acc: 0.9949 - val_loss: 0.0925 - val_acc: 0.9880\n",
      "Epoch 192/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.1133 - val_acc: 0.9820\n",
      "Epoch 193/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0117 - acc: 0.9961 - val_loss: 0.1641 - val_acc: 0.9880\n",
      "Epoch 194/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0390 - acc: 0.9936 - val_loss: 0.1807 - val_acc: 0.9820\n",
      "Epoch 195/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.1444 - val_acc: 0.9760\n",
      "Epoch 196/200\n",
      "779/779 [==============================] - 0s 37us/step - loss: 0.0257 - acc: 0.9923 - val_loss: 0.1066 - val_acc: 0.9880\n",
      "Epoch 197/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0236 - acc: 0.9936 - val_loss: 0.1000 - val_acc: 0.9880\n",
      "Epoch 198/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0342 - acc: 0.9936 - val_loss: 0.1005 - val_acc: 0.9880\n",
      "Epoch 199/200\n",
      "779/779 [==============================] - 0s 40us/step - loss: 0.0115 - acc: 0.9961 - val_loss: 0.2023 - val_acc: 0.9701\n",
      "Epoch 200/200\n",
      "779/779 [==============================] - 0s 38us/step - loss: 0.0111 - acc: 0.9961 - val_loss: 0.1024 - val_acc: 0.9880\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               25088     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 35,523\n",
      "Trainable params: 35,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1 = FFNN()\n",
    "p1.graphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "18bdb72cf1ec24c25ad4d3c22b30e7f2dcf43e32"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "d30c822c0559ff89cd839be084265bdc6ba82476"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "d751a52a09a49c3e2e3790ce7340d79c06996bf4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
